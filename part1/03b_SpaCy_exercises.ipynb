{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sensitive-bristol",
   "metadata": {},
   "source": [
    "# Exercise - 02\n",
    "\n",
    "Working with __SpaCy__ \n",
    "\n",
    "Credit: Adapted from SpaCy.io\n",
    "\n",
    "This work refers to v2.x with 3.0 release on the anvil, however, the key changes for 3.0 are not impacting the core API, rather, making Transformer pipelines feasible. \n",
    "\n",
    "We will cover Transformer as a separate module with Huggingface in a later session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "varied-instrumentation",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.2 MB 5.5 MB/s \n",
      "\u001b[?25hCollecting pathy>=0.3.5\n",
      "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 924 kB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /home/yashroff/.cache/pypoetry/virtualenvs/ds-kO09CnjC-py3.8/lib/python3.8/site-packages (from spacy) (21.0)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.1-py3-none-any.whl (7.0 kB)\n",
      "Collecting wasabi<1.1.0,>=0.8.1\n",
      "  Downloading wasabi-0.8.2-py3-none-any.whl (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.6-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
      "Requirement already satisfied: jinja2 in /home/yashroff/.cache/pypoetry/virtualenvs/ds-kO09CnjC-py3.8/lib/python3.8/site-packages (from spacy) (3.0.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/yashroff/.cache/pypoetry/virtualenvs/ds-kO09CnjC-py3.8/lib/python3.8/site-packages (from spacy) (4.62.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/yashroff/.cache/pypoetry/virtualenvs/ds-kO09CnjC-py3.8/lib/python3.8/site-packages (from spacy) (1.19.5)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.6-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "\u001b[K     |████████████████████████████████| 130 kB 1.1 MB/s \n",
      "\u001b[?25hCollecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.1\n",
      "  Downloading srsly-2.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (454 kB)\n",
      "\u001b[K     |████████████████████████████████| 454 kB 2.8 MB/s \n",
      "\u001b[?25hCollecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: setuptools in /home/yashroff/.cache/pypoetry/virtualenvs/ds-kO09CnjC-py3.8/lib/python3.8/site-packages (from spacy) (58.1.0)\n",
      "Collecting blis<0.8.0,>=0.4.0\n",
      "  Downloading blis-0.7.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.9 MB 4.6 MB/s \n",
      "\u001b[?25hCollecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[K     |████████████████████████████████| 181 kB 6.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /home/yashroff/.cache/pypoetry/virtualenvs/ds-kO09CnjC-py3.8/lib/python3.8/site-packages (from spacy) (1.8.2)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.8\n",
      "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
      "Collecting typer<0.5.0,>=0.3.0\n",
      "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/yashroff/.cache/pypoetry/virtualenvs/ds-kO09CnjC-py3.8/lib/python3.8/site-packages (from spacy) (2.26.0)\n",
      "Collecting thinc<8.1.0,>=8.0.12\n",
      "  Downloading thinc-8.0.13-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (636 kB)\n",
      "\u001b[K     |████████████████████████████████| 636 kB 1.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /home/yashroff/.cache/pypoetry/virtualenvs/ds-kO09CnjC-py3.8/lib/python3.8/site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Collecting smart-open<6.0.0,>=5.0.0\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 2.6 MB/s \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /home/yashroff/.cache/pypoetry/virtualenvs/ds-kO09CnjC-py3.8/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (3.7.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/yashroff/.cache/pypoetry/virtualenvs/ds-kO09CnjC-py3.8/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/yashroff/.cache/pypoetry/virtualenvs/ds-kO09CnjC-py3.8/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/yashroff/.cache/pypoetry/virtualenvs/ds-kO09CnjC-py3.8/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/yashroff/.cache/pypoetry/virtualenvs/ds-kO09CnjC-py3.8/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.7)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/yashroff/.cache/pypoetry/virtualenvs/ds-kO09CnjC-py3.8/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/yashroff/.cache/pypoetry/virtualenvs/ds-kO09CnjC-py3.8/lib/python3.8/site-packages (from jinja2->spacy) (2.0.1)\n",
      "Installing collected packages: murmurhash, cymem, catalogue, wasabi, typer, srsly, smart-open, preshed, blis, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy\n",
      "Successfully installed blis-0.7.5 catalogue-2.0.6 cymem-2.0.6 langcodes-3.3.0 murmurhash-1.0.6 pathy-0.6.1 preshed-3.0.6 smart-open-5.2.1 spacy-3.2.0 spacy-legacy-3.0.8 spacy-loggers-1.0.1 srsly-2.4.2 thinc-8.0.13 typer-0.4.0 wasabi-0.8.2\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/yashroff/.cache/pypoetry/virtualenvs/ds-kO09CnjC-py3.8/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "2021-11-16 00:11:27.530780: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-16 00:11:27.531238: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Collecting en-core-web-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.9 MB 281 kB/s \n",
      "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /home/yashroff/.cache/pypoetry/virtualenvs/ds-kO09CnjC-py3.8/lib/python3.8/site-packages (from en-core-web-sm==3.2.0) (3.2.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/yashroff/.cache/pypoetry/virtualenvs/ds-kO09CnjC-py3.8/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/yashroff/.cache/pypoetry/virtualenvs/ds-kO09CnjC-py3.8/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /home/yashroff/.cache/pypoetry/virtualenvs/ds-kO09CnjC-py3.8/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/yashroff/.cache/pypoetry/virtualenvs/ds-kO09CnjC-py3.8/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /home/yashroff/.cache/pypoetry/virtualenvs/ds-kO09CnjC-py3.8/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.13)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/yashroff/.cache/pypoetry/virtualenvs/ds-kO09CnjC-py3.8/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/yashroff/.cache/pypoetry/virtualenvs/ds-kO09CnjC-py3.8/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/yashroff/.cache/pypoetry/virtualenvs/ds-kO09CnjC-py3.8/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /home/yashroff/.cache/pypoetry/virtualenvs/ds-kO09CnjC-py3.8/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/yashroff/.cache/pypoetry/virtualenvs/ds-kO09CnjC-py3.8/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /home/yashroff/.cache/pypoetry/virtualenvs/ds-kO09CnjC-py3.8/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.8)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/yashroff/.cache/pypoetry/virtualenvs/ds-kO09CnjC-py3.8/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.19.5)\n",
      "Requirement already satisfied: jinja2 in /home/yashroff/.cache/pypoetry/virtualenvs/ds-kO09CnjC-py3.8/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/yashroff/.cache/pypoetry/virtualenvs/ds-kO09CnjC-py3.8/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/yashroff/.cache/pypoetry/virtualenvs/ds-kO09CnjC-py3.8/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.62.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /home/yashroff/.cache/pypoetry/virtualenvs/ds-kO09CnjC-py3.8/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.8.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/yashroff/.cache/pypoetry/virtualenvs/ds-kO09CnjC-py3.8/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.26.0)\n",
      "Requirement already satisfied: setuptools in /home/yashroff/.cache/pypoetry/virtualenvs/ds-kO09CnjC-py3.8/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (58.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/yashroff/.cache/pypoetry/virtualenvs/ds-kO09CnjC-py3.8/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/yashroff/.cache/pypoetry/virtualenvs/ds-kO09CnjC-py3.8/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/yashroff/.cache/pypoetry/virtualenvs/ds-kO09CnjC-py3.8/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /home/yashroff/.cache/pypoetry/virtualenvs/ds-kO09CnjC-py3.8/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/yashroff/.cache/pypoetry/virtualenvs/ds-kO09CnjC-py3.8/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.7.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/yashroff/.cache/pypoetry/virtualenvs/ds-kO09CnjC-py3.8/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/yashroff/.cache/pypoetry/virtualenvs/ds-kO09CnjC-py3.8/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/yashroff/.cache/pypoetry/virtualenvs/ds-kO09CnjC-py3.8/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/yashroff/.cache/pypoetry/virtualenvs/ds-kO09CnjC-py3.8/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.7)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/yashroff/.cache/pypoetry/virtualenvs/ds-kO09CnjC-py3.8/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/yashroff/.cache/pypoetry/virtualenvs/ds-kO09CnjC-py3.8/lib/python3.8/site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.2.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/yashroff/.cache/pypoetry/virtualenvs/ds-kO09CnjC-py3.8/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "#python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-marathon",
   "metadata": {},
   "source": [
    "* Import the English language class\n",
    "* Create the `nlp` object that contains:\n",
    "    * the processing pipeline and\n",
    "    * includes language-specific rules for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "simple-score",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-16 00:11:50.213489: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-16 00:11:50.213555: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "periodic-contrary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the English language class\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Create the nlp object that will contain the processing pipeline and includes the language specific rules for tokenization, etc.\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-grocery",
   "metadata": {},
   "source": [
    "When you process a text with the nlp object, spaCy creates a Doc object – short for \"document\". The Doc lets you access information about the text in a structured way, and no information is lost.\n",
    "\n",
    "The Doc behaves like a normal Python sequence by the way and lets you iterate over its tokens, or get a token by its index. But more on that later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "strange-medicine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "world\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "# Created by processing a string of text with the nlp object\n",
    "doc = nlp(\"Hello world!\")\n",
    "\n",
    "# Iterate over tokens in a Doc\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offshore-teens",
   "metadata": {},
   "source": [
    "## Token objects, Lexemes, & Hashes\n",
    "\n",
    "![SpaCy Arch](images/vocab_stringstore.png)\n",
    "\n",
    "* Represent the tokens in a document – for example, a word or a punctuation character.\n",
    "* To get a token at a specific position, you can index into the doc.\n",
    "* also provide various attributes that let you access more information about the tokens. For example, the .text attribute returns the verbatim token text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "balanced-tiffany",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world!\n",
      "world\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Hello world!\")\n",
    "\n",
    "# print the text\n",
    "print(doc.text)\n",
    "\n",
    "# Index into the Doc to get a single Token\n",
    "token = doc[1]\n",
    "\n",
    "# Get the token text via the .text attribute\n",
    "print(token.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "suitable-transmission",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1703489418272052182\n",
      "world\n",
      "world 1703489418272052182 True\n"
     ]
    }
   ],
   "source": [
    "# SpaCy shares words as hash objects\n",
    "print(nlp.vocab.strings[\"world\"])\n",
    "print(nlp.vocab.strings[1703489418272052182])\n",
    "\n",
    "# A Lexeme object is an entry in the vocabulary\n",
    "lexeme = nlp.vocab[\"world\"]\n",
    "print(lexeme.text, lexeme.orth, lexeme.is_alpha)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "presidential-corps",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-creativity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.__ import _____\n",
    "nlp_de = ____\n",
    "doc = nlp_de(\"Viel Spaß beim Codieren\")\n",
    "print(___)\n",
    "\n",
    "# print the second token\n",
    "token2 = ___\n",
    "print(___)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charitable-adventure",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "You can use `token.like_num` to find numbers in the text. Find $ values in this exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinate-policy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the text\n",
    "doc = nlp(\n",
    "    \"According to the BigMac price index, the cost of a Big Mac is cheapest in Turkey at $1.74, while the cost in Switzerland is highest at $7.01. In the US, it is about $5.75\"\n",
    ")\n",
    "\n",
    "# Iterate over the tokens in the doc\n",
    "for ___ in doc:\n",
    "    # Check if the token resembles a number\n",
    "    if token.___:\n",
    "        # is it a USD currency\n",
    "        prev_token = doc[token.i - 1]\n",
    "        if ____.___ == \"$\":\n",
    "            print(\"Price found: $\", _____, sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-conversion",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Let's move into using pre-trained model packages\n",
    "\n",
    "\n",
    "**POS** and **NER** tagging \n",
    "Models, though trained on large corpus of labeled texts, can be updated with specific examples to fine-tune their predictions. \n",
    "\n",
    "1. Download the pre-trained model package, `en_core_web_sm` (trained on web corpus)\n",
    "2. Load using `spacy.load`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spread-welcome",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\") # contains pipeline, vocabulary, and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-saturn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS using the pre-trained model\n",
    "\n",
    "# Process the text - create tokens\n",
    "doc = nlp(\"They went to the Eiffel Tower for sight-seeing and ate a pizza that cost $5.54 per slice.\")\n",
    "# Iterate over tokens\n",
    "for token in doc:\n",
    "    print(f\"{token.___:<12}{token.___:15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mysterious-fusion",
   "metadata": {},
   "source": [
    "## Pro-tip\n",
    "\n",
    "Get explanations of the most common tags & labels using `spacy.explain(tag)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-orlando",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"POS:\", spacy.explain('POS'))\n",
    "print(\"GPE:\", spacy.explain('GPE')) # Geopolitical entity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sacred-municipality",
   "metadata": {},
   "source": [
    "### Exercise 3: Syntactic Dependencies\n",
    "* The `dep` attribute returns the predicted dependency label. \n",
    "* the `head` attribute returns the parent token \n",
    "* Predict the entity and their labels\n",
    "NOTE: Any attribute that returns a text is followed by an underscore _, for instance, `ent.label_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-piece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the token text, position, dependencies and parent (head)\n",
    "for token in doc[:5]:\n",
    "    print(token.text, token.___, token.___, token.___.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threaded-september",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.___, ent.___)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-westminster",
   "metadata": {},
   "source": [
    "## Rule based matching\n",
    "\n",
    "### Using `Matcher`\n",
    "\n",
    "`Matcher` helps us find patterns in our text. \n",
    "* First initialize `Matcher` with the object's vocabulary, `vocab`\n",
    "* The `add` attribute lets you add a pattern and uses 3 inputs: a unique ID, a callback function - `None` in our case, and a pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-joining",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "# initialize the matcher with the shared vocabulary\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# add the pattern to the matcher\n",
    "pattern = [{\"TEXT\": \"12\"}, {\"TEXT\": \"Angry\"},{\"TEXT\":\"Men\"}]\n",
    "matcher.add(\"MOVIE_PATTERN\", [pattern])\n",
    "\n",
    "doc = nlp(\"We watched 12 Angry Men together\")\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-retention",
   "metadata": {},
   "outputs": [],
   "source": [
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-convergence",
   "metadata": {},
   "source": [
    "## Exercise 4: Complex Pattern Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-elevation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\n",
    "    \"After making the iOS update you won't notice a radical system-wide \"\n",
    "    \"redesign: nothing like the aesthetic upheaval we got with iOS 7. Most of \"\n",
    "    \"iOS 11's furniture remains the same as in iOS 10. But you will discover \"\n",
    "    \"some tweaks once you delve a little deeper.\"\n",
    ")\n",
    "\n",
    "# Write a pattern for full iOS versions (\"iOS 7\", \"iOS 11\", \"iOS 10\")\n",
    "pattern = [[{\"TEXT\": \"iOS\"}, {\"IS_DIGIT\": True}]]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add(\"IOS_VERSION_PATTERN\", pattern)\n",
    "matches = matcher(doc)\n",
    "print(\"Total matches found:\", len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Match found:\", doc[start:end].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-column",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\n",
    "    \"A new study on the aerosol and surface stability of SARS-CoV 2 compared to SARS-CoV 1 that was published\"\n",
    "    \"by the New England Journal of Medicine reveals that SARS-CoV 2 can stay suspended in the air for three hours, \"\n",
    "    \"with a similar reduction in its infectious rate as that of SARS-CoV 1\"\n",
    ")\n",
    "\n",
    "# Write a pattern to match all instances of SARS-CoV 1 and SARS-CoV 2\n",
    "pattern = [[{\"TEXT\": \"CoV\"}, {\"IS_DIGIT\": True}]]\n",
    "pattern2 = [{\"LOWER\": \"sars\"}, {\"TEXT\": \"-\"}, {\"LOWER\": \"cov\"}, {\"TEXT\": \"-\"}, {\"IS_DIGIT\": True}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add(\"VIRUS_TYPE_PATTERN\", pattern)\n",
    "matches = matcher(doc)\n",
    "print(\"Total matches found:\", len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Match found:\", doc[start:end].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enormous-exploration",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(f\"{token.text:<10}{token.pos_:<10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joined-observer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize with the shared vocab\n",
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Patterns are lists of dictionaries describing the tokens\n",
    "pattern = [{\"LEMMA\": \"love\", \"POS\": \"VERB\", \"LOWER\": \"cats\"}]\n",
    "matcher.add(\"LOVE_CATS\", [pattern])\n",
    "\n",
    "# Operators can specify how often a token should be matched\n",
    "pattern = [{\"TEXT\": \"very\", \"OP\": \"+\"}, {\"TEXT\": \"happy\"}]\n",
    "matcher.add(\"VERY_HAPPY\", [pattern])\n",
    "\n",
    "# Calling matcher on doc returns list of (match_id, start, end) tuples\n",
    "doc = nlp(\"I love cats and I'm very very happy\")\n",
    "matches = matcher(doc)\n",
    "print(len(matches))\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    print(doc[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continental-spring",
   "metadata": {},
   "source": [
    "## Phrase Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-investigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "terms = [\"Barack Obama\", \"Angela Merkel\", \"Washington, D.C.\"]\n",
    "# Only run nlp.make_doc to speed things up\n",
    "patterns = [nlp.make_doc(text) for text in terms]\n",
    "matcher.add(\"TerminologyList\", patterns)\n",
    "\n",
    "doc = nlp(\"German Chancellor Angela Merkel and US President Barack Obama \"\n",
    "          \"converse in the Oval Office inside the White House in Washington, D.C.\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    print(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stopped-science",
   "metadata": {},
   "outputs": [],
   "source": [
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plain-layer",
   "metadata": {},
   "source": [
    "## Lemma + Proper Noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clinical-buying",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "doc = nlp(\n",
    "    \"i downloaded Fortnite on my laptop and can't open the game at all. Help? \"\n",
    "    \"so when I was downloading Minecraft, I got the Windows version where it \"\n",
    "    \"is the '.zip' folder and I used the default program to unpack it... do \"\n",
    "    \"I also need to download Winzip?\"\n",
    ")\n",
    "patterns = [{\"LEMMA\": \"download\"}, {\"POS\": \"PROPN\"}]\n",
    "matcher.add(\"DrivePattern\",[pattern])\n",
    "matches = matcher(doc)\n",
    "\n",
    "print(\"Total matches found:\", len(matches))\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    print(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "traditional-habitat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's validate if this makes sense:\n",
    "# Remember, you can use spacy.explain(\"TAG\")\n",
    "\n",
    "for token in doc:\n",
    "    if token.lemma_ == \"download\":\n",
    "        next_token = doc[token.i + 1]\n",
    "        print(token.lemma_, \"POS:\", next_token.pos_) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-merchant",
   "metadata": {},
   "source": [
    "# Document Similarity\n",
    "\n",
    "* spaCy can compare two documents, spans, or tokens\n",
    "* each have a `.similarity` method that takes another object as input\n",
    "* retuns similarity score between 0 & 1\n",
    "\n",
    "NOTE: Cannot use with `en_core_web_sm` -- you need a model with word vectors included\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-trade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a larger model with vectors\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Compare two documents\n",
    "doc1 = nlp(\"I like fast food\")\n",
    "doc2 = nlp(\"I like pizza\")\n",
    "doc3 = nlp(\"The car was old\")\n",
    "span = nlp(\"They baked cookies and muffins\")[2:5]\n",
    "print(doc1.similarity(doc2))\n",
    "print(doc2.similarity(doc3))\n",
    "print(doc2.similarity(span))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demonstrated-starter",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the word vector of a document\n",
    "doc = nlp(\"The king was furious\") # would this change if you modified the sentence?\n",
    "print(doc[1].vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-horizon",
   "metadata": {},
   "source": [
    "# Bigrams with SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "canadian-today",
   "metadata": {},
   "source": [
    "Unigrams, Bi-Grams, Tri-grams, and N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "french-advantage",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alpha_tokens(doc):\n",
    "    sentence = list()\n",
    "    for token in doc:\n",
    "        if token.is_alpha:\n",
    "            sentence.append(token)            \n",
    "    return sentence\n",
    "\n",
    "def get_bigram(doc):\n",
    "    bigrams = list()\n",
    "    sentence = get_alpha_tokens(doc)\n",
    "    \n",
    "    for index in range(len(sentence)-1):\n",
    "        word1 = sentence[index]\n",
    "        word2 = sentence[index+1]\n",
    "        bigram = (word1, word2)\n",
    "        bigrams.append(bigram)\n",
    "    \n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exotic-colony",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(\"It was a dark and cloudy day yesterday\")\n",
    "\n",
    "bigrams = get_bigram(doc)\n",
    "for token1, token2 in bigrams:\n",
    "    print(token1.text, token2.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-quarter",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "\n",
    "Using `SpaCy` write a function that returns n-grams of a document. You may use the bigram as reference. Helper functions are also available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compact-vanilla",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram(doc, n=2):\n",
    "\n",
    "    <<YOUR CODE HERE>> # Fill this\n",
    "    \n",
    "    return ngrams\n",
    "\n",
    "def print_ngrams(ngrams):\n",
    "    n = len(ngrams[0])\n",
    "    for ngram in ngrams:\n",
    "        print(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signal-category",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(\"It was a dark and cloudy day yesterday\")\n",
    "\n",
    "ngrams = get_ngram(doc, n=2)\n",
    "if len(ngrams) > 0:\n",
    "    print_ngrams(ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "former-garage",
   "metadata": {},
   "source": [
    "## Exercise 6\n",
    "\n",
    "Process the text to only print verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-testament",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporate-skating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the texts and print the adjectives\n",
    "\n",
    "TEXTS = ['We went kayaking last week', 'It was hard to drive on the road due to rain', 'Can you come here please?', \"What's taking you so long?\"]\n",
    "for text in TEXTS:\n",
    "    doc = nlp(text)\n",
    "    print([token.text for token in doc if <<YOUR CODE HERE>>]) # Fill this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indonesian-estimate",
   "metadata": {},
   "source": [
    "## Exercise 7\n",
    "\n",
    "Print the entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tired-enhancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "\n",
    "json_text = '''\n",
    "[\n",
    "    \"McDonalds is my favorite restaurant.\",\n",
    "    \"Here I thought @McDonalds only had precooked burgers but it seems they only have not cooked ones?? I have no time to get sick..\",\n",
    "    \"People really still eat McDonalds :(\",\n",
    "    \"The McDonalds in Spain has chicken wings. My heart is so happy \",\n",
    "    \"@McDonalds Please bring back the most delicious fast food sandwich of all times!!....The Arch Deluxe :P\",\n",
    "    \"please hurry and open. I WANT A #McRib SANDWICH SO BAD! :D\",\n",
    "    \"This morning i made a terrible decision by gettin mcdonalds and now my stomach is payin for it\"\n",
    "]'''\n",
    "TEXTS = json.loads(json_text)\n",
    "\n",
    "# Process the texts and print the entities\n",
    "docs = [nlp(text) for text in TEXTS]\n",
    "entities = [<<YOUR CODE HERE>>] # Fill this\n",
    "print(*entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-fitting",
   "metadata": {},
   "source": [
    "## Solution 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-motor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.de import German\n",
    "nlp_de = German()\n",
    "doc = nlp_de(\"Viel Spaß beim Codieren\")\n",
    "print(doc.text)\n",
    "\n",
    "# print the second token\n",
    "token2 = doc[1]\n",
    "print(token2.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-attack",
   "metadata": {},
   "source": [
    "## Solution 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-cooperation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the text\n",
    "doc = nlp(\n",
    "    \"According to the BigMac price index, the cost of a Big Mac is cheapest in Turkey at $1.74, while the cost in Switzerland is highest at $7.01. In the US, it is about $5.75\"\n",
    ")\n",
    "\n",
    "# Iterate over the tokens in the doc\n",
    "for token in doc:\n",
    "    # Check if the token resembles a number\n",
    "    if token.like_num:\n",
    "        # Get the next token in the document\n",
    "        prev_token = doc[token.i - 1]\n",
    "        # Check if the previous token's text equals \"$\"\n",
    "        if prev_token.text == \"$\":\n",
    "            print(\"Price found: $\", token.text, sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sporting-crack",
   "metadata": {},
   "source": [
    "## Solution 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-planning",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc[:5]:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technical-tobacco",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text:<35}{ent.label_:<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-dependence",
   "metadata": {},
   "source": [
    "## Solution 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "still-location",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram(doc, n=2):\n",
    "    ngrams = list()\n",
    "    sentence = get_alpha_tokens(doc)\n",
    "    \n",
    "    for index in range(len(sentence) - n):\n",
    "        ngram = tuple(sentence[index:index+n])\n",
    "        ngrams.append(ngram)\n",
    "    \n",
    "    return ngrams\n",
    "\n",
    "def print_ngrams(ngrams):\n",
    "    n = len(ngrams[0])\n",
    "    for ngram in ngrams:\n",
    "        print(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flexible-kidney",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(\"It was a dark and cloudy day yesterday\")\n",
    "\n",
    "ngrams = get_ngram(doc, n=7)\n",
    "if len(ngrams) > 0:\n",
    "    print_ngrams(ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-klein",
   "metadata": {},
   "source": [
    "## Solution 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "private-evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the texts and print the adjectives\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "TEXTS = ['We went kayaking last week', 'It was hard to drive on the road due to rain', 'Can you come here please?', \"What's taking you so long?\"]\n",
    "for text in TEXTS:\n",
    "    doc = nlp(text)\n",
    "    print([token.text for token in doc if token.pos_ == \"VERB\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-missile",
   "metadata": {},
   "source": [
    "## Solution 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-driver",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "\n",
    "json_text = '''\n",
    "[\n",
    "    \"McDonalds is my favorite restaurant.\",\n",
    "    \"Here I thought @McDonalds only had precooked burgers but it seems they only have not cooked ones?? I have no time to get sick..\",\n",
    "    \"People really still eat McDonalds :(\",\n",
    "    \"The McDonalds in Spain has chicken wings. My heart is so happy \",\n",
    "    \"@McDonalds Please bring back the most delicious fast food sandwich of all times!!....The Arch Deluxe :P\",\n",
    "    \"please hurry and open. I WANT A #McRib SANDWICH SO BAD! :D\",\n",
    "    \"This morning i made a terrible decision by gettin mcdonalds and now my stomach is payin for it\"\n",
    "]'''\n",
    "TEXTS = json.loads(json_text)\n",
    "\n",
    "# Process the texts and print the entities\n",
    "docs = [nlp(text) for text in TEXTS]\n",
    "entities = [doc.ents for doc in docs]\n",
    "print(*entities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
